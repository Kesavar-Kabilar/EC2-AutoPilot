# Machine Problem 2: Load Balancing + Auto Scaling

This assignment focused on implementing load balancing and auto-scaling using Amazon Web Services (AWS) EC2.

## Assignment Overview

The assignment was divided into two sections:

1. **AWS Elastic Load Balancing (ELB)**: I set up a load balancer to distribute traffic between two web servers hosting a simple seed value application on Amazon EC2 instances.
2. **Amazon EC2 Auto Scaling**: I created an auto-scaling group to dynamically adjust the number of EC2 instances based on CPU usage, hosting a slightly different application that responds with its IP address and can trigger CPU stress.

## Solution Implementation

### Section 1: Load Balancing

* **Web Application (Seed Value Server):** I developed a simple web server application in Python using the Flask framework. This application manages a single integer value, the "seed."  It handles two HTTP requests:
    * **POST `/` with JSON payload `{"num": 100}`:** This request updates the seed value. The `num` value in the JSON payload can be any integer. My Flask application parses this JSON payload using `request.get_json()`, extracts the integer, and updates the stored seed value.
    * **GET `/`:** This request retrieves the current seed value.  My Flask application converts the integer seed value to a string using `str()` and returns it in the response body.  It's important that this is returned as a plain string and *not* as JSON.
* **EC2 Instances**: I launched two identical EC2 instances using the latest Amazon Linux AMI. I chose the smallest instance type (t2.nano or t2.micro) to minimize costs. I connected to both instances via SSH using my terminal. I installed Python and Flask on both instances using `sudo yum update -y` followed by `sudo yum install python3 -y` and `pip3 install flask`. I then deployed my web application to each instance, making sure it was running on the same port (e.g., 5000). I configured the inbound firewall rules (security groups) on the EC2 instances to allow public access to port 5000.  Specifically, I added a rule to the security group associated with my EC2 instances that allowed inbound traffic on port 5000 from 0.0.0.0/0 (anywhere).
* **Load Balancer**: I created an AWS Application Load Balancer.  This type of load balancer is specifically designed for HTTP/HTTPS traffic, which is what my application uses. I configured the load balancer to listen on port 80 for HTTP traffic. This means that when a user accesses the load balancer's DNS name on port 80, the load balancer will accept the connection. I created a target group, which acts as a container for the EC2 instances that will receive traffic. I registered the two EC2 instances with this target group, specifying port 5000 as the target port. This tells the load balancer to forward traffic it receives on port 80 to port 5000 on the registered instances.  I set the target group's health check ping path to "/" to match my web application's root path. The health check is crucial: the load balancer periodically sends requests to the specified path on the EC2 instances to ensure they are healthy and responding. If an instance fails the health check, the load balancer will stop sending traffic to it. I created a custom security group for the load balancer that allowed inbound HTTP traffic from anywhere (0.0.0.0/0) to ensure the autograder could access my application. I did *not* use the default security group.  This security group is associated with the load balancer itself and controls what traffic is allowed to reach the load balancer.
* **Load Balancing Details:** The Application Load Balancer distributes incoming HTTP requests across the registered EC2 instances in a round-robin fashion by default. This helps to prevent any single instance from being overloaded and improves the overall availability and responsiveness of the application. The load balancer also handles connection management, SSL termination (if configured), and other tasks, freeing up the web servers to focus on running the application.
* **Testing**: I tested the load balancer by accessing its DNS name in my browser. I confirmed that the load balancer distributed requests between the two EC2 instances and that I could successfully update and retrieve the seed value by sending POST and GET requests. I used `curl` from my local machine to send test requests: `curl -X POST -H "Content-Type: application/json" -d '{"num": 42}' http://<load balancer DNS>/` and `curl http://<load balancer DNS>/`.  I observed that subsequent requests were handled by different EC2 instances, demonstrating that the load balancing was working as expected.

### Section 2: Auto Scaling

* **HTTP Server (`serve.py`):** I created a separate Python Flask application, `serve.py`.  This server has two endpoints:
    * **POST `/`:** This endpoint is designed to trigger CPU stress. When a POST request is received, the server uses `subprocess.Popen()` to launch the `stress_cpu.py` script in a *separate process*. This is crucial because it allows the Flask server to remain responsive and handle other requests while `stress_cpu.py` is running.  I used `Popen` with `stdout=subprocess.PIPE` and `stderr=subprocess.PIPE` so I could optionally capture the output of `stress_cpu.py` for debugging, though I didn't end up needing it for this assignment.
    * **GET `/`:** This endpoint returns the private IP address of the EC2 instance. I used `socket.gethostname()` to get the hostname, which usually resolves to the private IP address within the AWS environment.
* **Launch Template**: I created a launch template in AWS. This template defines the configuration for the EC2 instances that will be launched by the auto-scaling group.  In the "User data" section of the template, I added a bash script. This script does the following:
    * Updates the system packages (`sudo yum update -y`).
    * Installs `stress-ng`, `htop`, Python 3 (`sudo yum install python3 -y`), `pip`, `flask`, and `git`.  `stress-ng` is used for simulating CPU load, and `htop` is a useful interactive process viewer for monitoring CPU usage.
    * Clones my web server code from my *private* GitHub repository.  I included my GitHub personal access token in the clone URL: `git clone https://<personal-access-token>@github.com/username/repository_name`.  This is essential for automating the code deployment.
    * Navigates to the cloned repository directory.
    * Starts my `serve.py` application using `python3 serve.py`.
* **Auto Scaling Group**: I created an auto-scaling group and configured it to use the launch template I created. I set the minimum size to 0, the maximum size to 3, and the desired capacity to 1. This means that the auto-scaling group will always try to maintain at least 0 instances, will never launch more than 3 instances, and will start with 1 instance. I configured the auto-scaling group to be associated with a *new* internet-facing load balancer (separate from the one in Section 1).  This separate load balancer is important for isolating the two sections of the assignment.
* **Auto Scaling Details:** The auto-scaling group monitors the CPU utilization of the EC2 instances.  It uses the CloudWatch service to collect this metric.  Based on the scaling policy I defined, the auto-scaling group automatically adjusts the number of running instances.
* **Scaling Policy**: I configured a target tracking scaling policy based on CPU utilization. The target CPU utilization was set to 50%. This means that the auto-scaling group will try to keep the average CPU utilization of the running instances as close to 50% as possible. The policy adds one instance when the average CPU utilization exceeds 50% and removes one instance when it drops below 50%. The cooldown period was set to 300 seconds to prevent rapid scaling events. The cooldown period is the amount of time the auto-scaling group waits after a scaling action before it considers another scaling action. This helps to prevent the auto-scaling group from overreacting to short-term fluctuations in CPU utilization.
* **Testing**: I tested the auto-scaling functionality by sending POST requests to the load balancer associated with the auto-scaling group. I monitored the CPU utilization of the instances using `htop` via SSH and verified that the auto-scaling group launched new instances when the CPU utilization exceeded 50%. I also confirmed that the newly launched instances automatically ran my `serve.py` application and responded with their IP addresses when I sent GET requests.  I used `curl` to send the POST requests: `curl -X POST http://<auto scaling load balancer DNS>/`. I sent multiple POST requests in quick succession to simulate a high load and observed the auto-scaling group launching additional instances.  I then monitored the CPU utilization and confirmed that the auto-scaling group scaled down the number of instances once the CPU utilization dropped below 50% and the cooldown period had elapsed.

## File Descriptions

* **`serve.py`**: This Python Flask application is the core of my auto-scaling setup. It handles POST requests to trigger CPU
